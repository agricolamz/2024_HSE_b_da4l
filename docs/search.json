[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "daR4hs",
    "section": "",
    "text": "О курсе\nМатериалы для курса Анализа данных для лингвистов, Школа лингвистики НИУ ВШЭ.",
    "crumbs": [
      "О курсе"
    ]
  },
  {
    "objectID": "index.html#используемые-пакеты",
    "href": "index.html#используемые-пакеты",
    "title": "daR4hs",
    "section": "Используемые пакеты",
    "text": "Используемые пакеты\n\npackageVersion(\"tidyverse\")\n\n[1] '2.0.0'\n\npackageVersion(\"fitdistrplus\")\n\n[1] '1.1.11'\n\npackageVersion(\"mixtools\")\n\n[1] '2.0.0'\n\npackageVersion(\"lme4\")\n\n[1] '1.1.35.1'\n\npackageVersion(\"lmerTest\")\n\n[1] '3.1.3'\n\npackageVersion(\"car\")\n\n[1] '3.1.2'\n\npackageVersion(\"pscl\")\n\n[1] '1.5.5.1'\n\npackageVersion(\"nnet\")\n\n[1] '7.3.19'\n\npackageVersion(\"MASS\")\n\n[1] '7.3.60'\n\npackageVersion(\"ggeffects\")\n\n[1] '1.3.4'\n\npackageVersion(\"brms\")\n\n[1] '2.20.4'\n\npackageVersion(\"tidybayes\")\n\n[1] '3.0.6'",
    "crumbs": [
      "О курсе"
    ]
  },
  {
    "objectID": "index.html#домашние-задания",
    "href": "index.html#домашние-задания",
    "title": "daR4hs",
    "section": "Домашние задания",
    "text": "Домашние задания\n\nдомашнее задание к лекции 13.02.2024:\n\nвспомните пожалуйста, условные вероятности, формулу Байеса и при каких условиях ее применяют;\nпосмотрите освежающие материалы про условную вероятность и формулу Байеса.",
    "crumbs": [
      "О курсе"
    ]
  },
  {
    "objectID": "01-distributions.html#распределения-в-r",
    "href": "01-distributions.html#распределения-в-r",
    "title": "1  Распределения",
    "section": "1.1 Распределения в R",
    "text": "1.1 Распределения в R\nВ R встроено какое-то количество известных распределений. Все они представлены четырьмя функциями:\n\nd... (функция плотности, probability density function),\np... (функция распределения, cumulative distribution function) — интеграл площади под кривой от начала до указанной квантили\nq... (обратная функции распределения, inverse cumulative distribution function) — значение p-той квантили распределения\nи r... (рандомные числа из заданного распределения).\n\nРассмотрим все это на примере нормального распределения.\n\ntibble(x = 1:100,\n       PDF = dnorm(x = x, mean = 50, sd = 10)) |&gt; \n  ggplot(aes(x, PDF))+\n  geom_point()+\n  geom_line()+\n  labs(title = \"PDF нормального распределения (μ = 50, sd = 10)\")\n\n\n\n\n\n\n\ntibble(x = 1:100,\n       CDF = pnorm(x, mean = 50, sd = 10)) |&gt; \n  ggplot(aes(x, CDF))+\n  geom_point()+\n  geom_line()+\n  labs(title = \"CDF нормального распределения (μ = 50, sd = 10)\")\n\n\n\n\n\n\n\ntibble(quantiles = seq(0, 1, by = 0.01),\n       value = qnorm(quantiles, mean = 50, sd = 10)) |&gt; \n  ggplot(aes(quantiles, value))+\n  geom_point()+\n  geom_line()+\n  labs(title = \"inverse CDF нормального распределения (μ = 50, sd = 10)\")\n\n\n\n\n\n\n\ntibble(sample = rnorm(100, mean = 50, sd = 10)) |&gt; \n  ggplot(aes(sample))+\n  geom_histogram()+\n  labs(title = \"выборка нормально распределенных чисел (μ = 50, sd = 10)\")\n\n\n\n\n\n\n\n\nЕсли не использовать set.seed(), то результат работы рандомизатора нельзя будет повторить.\n\n\n\n\n\n\nКакое значение имеет 25% квантиль нормального распределения со средним в 20 и стандартным отклонением 90? Ответ округлите до трех знаков после запятой.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nДанные из базы данных фонетических инвентарей PHOIBLE (Moran, McCloy, and Wright 2014), достаточно сильно упрощая, можно описать нормальным распределением со средним 35 фонем и стандартным отклонением 13. Если мы ничего не знаем про язык, оцените с какой вероятностью, согласно этой модели произвольно взятый язык окажется в промежутке между 25 и 50 фонемами? Ответ округлите до трех знаков после запятой.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nКакие есть недостатки у модели из предыдущего задания?\n\n\n\n\n\n\nответы:\nнедостаток 1\n\n\nнедостаток 2",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Распределения</span>"
    ]
  },
  {
    "objectID": "01-distributions.html#дискретные-переменные",
    "href": "01-distributions.html#дискретные-переменные",
    "title": "1  Распределения",
    "section": "1.2 Дискретные переменные",
    "text": "1.2 Дискретные переменные\n\n1.2.1 Биномиальное распределение\nБиномиальное распределение — распределение количетсва успехов эксперементов Бернулли из n попыток с вероятностью успеха p.\n\\[P(k | n, p) = \\frac{n!}{k!(n-k)!} \\times p^k \\times (1-p)^{n-k} =  {n \\choose k} \\times p^k \\times (1-p)^{n-k}\\] \\[ 0 \\leq p \\leq 1; n, k &gt; 0\\]\n\ntibble(x = 0:50,\n       density = dbinom(x = x, size = 50, prob = 0.16)) |&gt; \n  ggplot(aes(x, density))+\n  geom_point()+\n  geom_line()+\n  labs(title = \"Биномиальное распределение p = 0.16, n = 50\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nНемного упрощая данные из статьи (Rosenbach 2003: 394), можно сказать что носители британского английского предпочитают s-генитив (90%) of-генитиву (10%). Какова вероятность, согласно этим данным, что в интервью британского актера из 118 контекстов будет 102 s-генитивов? Ответ округлите до трёх ИЛИ МЕНЕЕ знаков после запятой.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nА какое значение количества s-генитивов наиболее ожидаемо, согласно этой модели?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.2.2 Геометрическое распределение\nГеометрическое распределение — распределение количетсва эксперементов Бернулли с вероятностью успеха p до первого успеха.\n\\[P(k | p) = (1-p)^k\\times p\\] \\[k\\in\\{1, 2, \\dots\\}\\]\n\ntibble(x = 0:50,\n       density = dgeom(x = x, prob = 0.16)) |&gt; \n  ggplot(aes(x, density))+\n  geom_point()+\n  geom_line()+\n  labs(title = \"Геометрическое распределение p = 0.16, n = 50\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nПриняв модель из (Rosenbach 2003: 394), какова вероятность, что в интервью с британским актером первый of-генитив будет третьим по счету?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.2.3 Распределение Пуассона\nРаспределение дискретной переменной, обозначающей количество случаев \\(k\\) некоторого события, которое происходит с некоторой заданной частотой \\(\\lambda\\).\n\\[P(\\lambda) = \\frac{e^{-\\lambda}\\times\\lambda^k}{k!}\\]\n\ntibble(k = 0:50,\n       density = dpois(x = k, lambda = 5)) |&gt; \n  ggplot(aes(k, density))+\n  geom_point()+\n  geom_line()+\n  labs(title = \"Распределение Пуассона с параметром λ = 5\")\n\n\n\n\n\n\n\n\nПараметр \\(\\lambda\\) в модели Пуассона одновременно является и средним, и дисперсией.\nПопробуем воспользоваться распределением Пуассона для моделирования количества слогов в андийском языке. Количество слогов – это всегда натуральное число (т. е. не бывает 2.5 слогов, не бывает -3 слогов и т. д., но в теории может быть 0 слогов), так что модель Пуассона здесь применима. Согласно модели Пуассона все слова независимо друг от друга получают сколько-то слогов согласно распределению Пуассона. Посмотрим на данные:\n\nandic_syllables &lt;- read_csv(\"https://raw.githubusercontent.com/agricolamz/2021_da4l/master/data/andic_syllables.csv\") \n\nandic_syllables |&gt; \n  ggplot(aes(n_syllables, count))+\n  geom_col()+\n  facet_wrap(~language, scales = \"free\")\n\n\n\n\n\n\n\n\nПтичка напела (мы научимся узнавать, откуда птичка это знает на следующем занятии), что андийские данные можно описать при помощи распределения Пуассона с параметром \\(\\lambda\\) = 2.783.\n\nandic_syllables |&gt; \n  filter(language == \"Andi\") |&gt; \n  rename(observed = count) |&gt; \n  mutate(predicted = dpois(n_syllables, lambda = 2.783)*sum(observed)) |&gt; \n  pivot_longer(names_to = \"type\", values_to = \"value\", cols = c(observed, predicted)) |&gt; \n  ggplot(aes(n_syllables, value, fill = type))+\n  geom_col(position = \"dodge\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nНа графиках ниже представлены предсказания трех Пуассоновских моделей, какая кажется лучше?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nВыше было написано:\n\nСогласно модели Пуассона все слова независимо друг от друга получают сколько-то слогов согласно распределению Пуассона.\n\nКакие проблемы есть у предположения о независимости друг от друга количества слогов разных слов в словаре?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Распределения</span>"
    ]
  },
  {
    "objectID": "01-distributions.html#числовые-переменные",
    "href": "01-distributions.html#числовые-переменные",
    "title": "1  Распределения",
    "section": "1.3 Числовые переменные",
    "text": "1.3 Числовые переменные\n\n1.3.1 Нормальное распределение\n\\[P(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\times e^{-\\frac{\\left(x-\\mu\\right)^2}{2\\sigma^2}}\\]\n\\[\\mu \\in \\mathbb{R}; \\sigma^2 &gt; 0\\]\n\ntibble(x = 1:100,\n       PDF = dnorm(x = x, mean = 50, sd = 10)) |&gt; \n  ggplot(aes(x, PDF))+\n  geom_point()+\n  geom_line()+\n  labs(title = \"PDF нормального распределения (μ = 50, sd = 10)\")\n\n\n\n\n\n\n\n\nПтичка напела, что длительность гласных американского английского из (Hillenbrand et al. 1995) можно описать нормальным распределением с параметрами \\(\\mu =\\) 274.673 и \\(\\sigma =\\) 64.482. Посмотрим, как можно совместить данные и это распределение:\n\nvowels &lt;- read_csv(\"https://raw.githubusercontent.com/agricolamz/2024_HSE_b_da4l/main/data/phonTools_hillenbrand_1995.csv\") \nvowels |&gt; \n  ggplot(aes(dur)) + \n  geom_histogram(aes(y = after_stat(density))) + # обратите внимание на аргумент after_stat(density)\n  stat_function(fun = dnorm, args = list(mean = 274.673, sd = 64.482), color = \"red\")\n\n\n\n\n\n\n\n\n\n\n1.3.2 Логнормальное распределение\n\\[P(x) = \\frac{1}{\\sqrt{x\\sigma2\\pi}}\\times e^{-\\frac{\\left(\\ln(x)-\\mu\\right)^2}{2\\sigma^2}}\\]\n\\[\\mu \\in \\mathbb{R}; \\sigma^2 &gt; 0\\]\n\ntibble(x = 1:100,\n       PDF = dlnorm(x = x, mean = 3, sd = 0.5)) |&gt; \n  ggplot(aes(x, PDF))+\n  geom_point()+\n  geom_line()+\n  labs(title = \"PDF логнормального распределения (μ = 3, σ = 0.5)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nКакая из логнормальных моделей для длительности гласных американского английского из (Hillenbrand et al. 1995) лучше подходит к данным? Попробуйте самостоятельно построить данный график.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.3.3 Что еще почитать про распределения?\nЛюди придумали очень много разных распределений. Стоит, наверное, также понимать, что распределения не существуют отдельно в вакууме: многие из них математически связаны друг с другом. Про это можно посмотреть вот здесь или здесь.\n\n\n\n\nHillenbrand, James, Laura A Getty, Michael J Clark, and Kimberlee Wheeler. 1995. “Acoustic Characteristics of American English Vowels.” The Journal of the Acoustical Society of America 97 (5): 3099–3111.\n\n\nMoran, Steven, Daniel McCloy, and Richard Wright, eds. 2014. PHOIBLE Online. Leipzig: Max Planck Institute for Evolutionary Anthropology. https://phoible.org/.\n\n\nRosenbach, Anette. 2003. “Aspects of Iconicity and Economy in the Choice Between the s-Genitive and the of-Genitive in English.” In Determinants of Grammatical Variation in English, edited by Günter Rohdenburg and Britta Mondorf. Berlin, New York: Mouton de Gruyter.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Распределения</span>"
    ]
  },
  {
    "objectID": "02-likelihood.html#оценка-вероятности",
    "href": "02-likelihood.html#оценка-вероятности",
    "title": "2  Метод максимального правдоподобия",
    "section": "2.1 Оценка вероятности",
    "text": "2.1 Оценка вероятности\n\nlibrary(tidyverse)\n\nКогда у нас задано некоторое распределение, мы можем задавать к нему разные вопросы. Например, если мы верим что длительность гласных американского английского из (Hillenbrand et al. 1995) можно описать логнормальным распределением с параметрами \\(\\ln{\\mu} =\\) 5.587 и \\(\\ln{\\sigma} =\\) 0.242, то мы можем делать некотрые предсказания относительно интересующей нас переменной.\n\nggplot() + \n  stat_function(fun = dlnorm, args = list(mean = 5.587, sd = 0.242))+\n  scale_x_continuous(breaks = 0:6*100, limits = c(0, 650))+\n  labs(x = \"длительность гласного (мс)\",\n       y = \"значение функции плотности\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nЕсли принять на веру, что логнормальное распределение с параметрами \\(\\ln{\\mu} =\\) 5.587 и \\(\\ln{\\sigma}=\\) 0.242 описывает данные длительности гласных американского английского из (Hillenbrand et al. 1995), то какова вероятность наблюдать значения между 300 и 400 мс? То же самое можно записать, используя математическую нотацию:\n\\[P\\left(X \\in [300,\\, 400] | X \\sim \\ln{\\mathcal{N}}(\\ln{\\mu} = 5.587, \\ln{\\sigma}=0.242)\\right) = ??\\] Ответ округлите до трех и меньше знаков после запятой.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nЕсли принять на веру, что биномиальное распределение с параметрами \\(p =\\) 0.9 описывает, согласно (Rosenbach 2003: 394) употребление s-генитивов в британском английском, то какова вероятность наблюдать значения между 300 и 350 генитивов в интервью, содержащее 400 генитивных контекстов? То же самое можно записать, используя математическую нотацию:\n\\[P\\left(X \\in [300,\\, 350] | X \\sim Binom(n = 400, p = 0.9)\\right) = ??\\] Ответ округлите до трех и меньше знаков после запятой.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Метод максимального правдоподобия</span>"
    ]
  },
  {
    "objectID": "02-likelihood.html#функция-правдоподобия",
    "href": "02-likelihood.html#функция-правдоподобия",
    "title": "2  Метод максимального правдоподобия",
    "section": "2.2 Функция правдоподобия",
    "text": "2.2 Функция правдоподобия\nЕсли при поиске вероятностей, мы предполагали, что данные нам неизвестны, а распределение и его параметры известны, то функция правдоподобия позволяет этот процесс перевернуть, запустив поиск параметров распределения, при изветсных данных и семье распределения:\n\\[L\\left(X \\sim Distr(...)|x\\right) = ...\\]\nТаким образом получается, что на основании функции плотности мы можем сравнивать, какой параметр лучше подходит к нашим данным.\nДля примера рассмотрим наш s-генетив: мы провели интервью и нам встретилось 85 s-генетивов из 100 случаев всех генетивов. Насколько хорошо подходит нам распределение с параметром p = 0.9?\n\n\n\n\n\n\n\n\n\nОтвет:\n\ndbinom(85, 100, 0.9)\n\n[1] 0.03268244\n\n\nПредставим теперь это как функцию от параметра p:\n\ntibble(p = seq(0, 1, by = 0.01)) |&gt; \n  ggplot(aes(p)) +\n  stat_function(fun = function(p) dbinom(85, 100, p), geom = \"col\")+\n  labs(x = \"параметр биномиального распределения p\",\n       y = \"значение функции правдоподобия\\n(одно наблюдение)\")\n\n\n\n\n\n\n\n\nА что если мы располагаем двумя интервью одного актера? В первом на сто генитивов пришлось 85 s-генитивов, а во втором – 89. В таком случае, также как и с вероятностью наступления двух независимых событий, значения функции плотности перемножаются.\n\ndbinom(85, 100, 0.9)*dbinom(89, 100, 0.9)\n\n[1] 0.003917892\n\n\n\ntibble(p = seq(0, 1, by = 0.01)) |&gt; \n  ggplot(aes(p)) +\n  stat_function(fun = function(p) dbinom(85, 100, p)*dbinom(89, 100, p), geom = \"col\")+\n  labs(x = \"параметр биномиального распределения p\",\n       y = \"значение функции правдоподобия\\n(два наблюдения)\")\n\n\n\n\n\n\n\n\nВ итоге:\n\nвероятность — P(data|distribution)\nправдоподобие — L(distribution|data)\n\nИнтеграл распределения/сумма значений вероятностей равен/на 1. Интеграл распределения/сумма значений правдоподобия может быть не равен/на 1.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Метод максимального правдоподобия</span>"
    ]
  },
  {
    "objectID": "02-likelihood.html#пример-с-непрерывным-распределением",
    "href": "02-likelihood.html#пример-с-непрерывным-распределением",
    "title": "2  Метод максимального правдоподобия",
    "section": "2.3 Пример с непрерывным распределением",
    "text": "2.3 Пример с непрерывным распределением\nМы уже обсуждали, что длительность гласных американского английского из (Hillenbrand et al. 1995) можно описать логнормальным распределением с параметрами \\(\\ln\\mu\\) и \\(\\ln\\sigma\\). Предположим, что \\(\\ln\\sigma = 0.342\\), построим функцию правдоподобия для \\(\\ln\\mu\\):\n\nvowels &lt;- read_csv(\"https://raw.githubusercontent.com/agricolamz/2024_HSE_b_da4l/main/data/phonTools_hillenbrand_1995.csv\") \n\ntibble(ln_mu = seq(5, 6, by = 0.001)) |&gt; \n  ggplot(aes(ln_mu)) + \n  stat_function(fun = function(ln_mu) dlnorm(vowels$dur[1], meanlog = ln_mu, sdlog = 0.242))+\n  labs(x = \"параметр логнормального распределения ln μ\",\n       y = \"значение функции правдоподобия\\n(одно наблюдение)\")\n\n\n\n\n\n\n\ntibble(ln_mu = seq(5, 6, by = 0.001)) |&gt; \n  ggplot(aes(ln_mu)) + \n  stat_function(fun = function(ln_mu) dlnorm(vowels$dur[1], meanlog = ln_mu, sdlog = 0.242)*dlnorm(vowels$dur[2], meanlog = ln_mu, sdlog = 0.242))+\n  labs(x = \"параметр логнормального распределения ln μ\",\n       y = \"значение функции правдоподобия\\n(два наблюдения)\")\n\n\n\n\n\n\n\ntibble(ln_mu = seq(5, 6, by = 0.001)) |&gt; \n  ggplot(aes(ln_mu)) + \n  stat_function(fun = function(ln_mu) dlnorm(vowels$dur[1], meanlog = ln_mu, sdlog = 0.242)*dlnorm(vowels$dur[2], meanlog = ln_mu, sdlog = 0.242)*dlnorm(vowels$dur[3], meanlog = ln_mu, sdlog = 0.242))+\n  labs(x = \"параметр логнормального распределения ln μ\",\n       y = \"значение функции правдоподобия\\n(три наблюдения)\")\n\n\n\n\n\n\n\n\nДля простоты в начале я зафиксировал один из параметров логнормального распредления: лог стандартное отклонение. Конечно, это совсем необязательно делать: можно создать матрицу значений лог среднего и лог стандартного отклонения и получить для каждой ячейки матрицы значения функции правдоподобия.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Метод максимального правдоподобия</span>"
    ]
  },
  {
    "objectID": "02-likelihood.html#метод-максимального-правдоподобия-mle",
    "href": "02-likelihood.html#метод-максимального-правдоподобия-mle",
    "title": "2  Метод максимального правдоподобия",
    "section": "2.4 Метод максимального правдоподобия (MLE)",
    "text": "2.4 Метод максимального правдоподобия (MLE)\nФункция правдоподобия позволяет подбирать параметры распределения. Оценка параметров распределения при помощи функции максимального правдоподобия получила название метод максимального правдоподобия. Его я и использовал ранее для того, чтобы получить значения распределений для заданий из первого занятия:\n\nданные длительности американских гласных из (Hillenbrand et al. 1995) и логнормальное распределение\n\n\nfitdistrplus::fitdist(vowels$dur, distr = 'lnorm', method = 'mle')\n\nFitting of the distribution ' lnorm ' by maximum likelihood \nParameters:\n         estimate  Std. Error\nmeanlog 5.5870359 0.005935135\nsdlog   0.2423978 0.004196453\n\n\n\nколичество андийских слогов в словах и распределение Пуассона\n\n\nandic_syllables &lt;- read_csv(\"https://raw.githubusercontent.com/agricolamz/2024_HSE_b_da4l/main/data/andic_syllables.csv\") \n\nandic_syllables |&gt; \n  filter(language == \"Andi\") |&gt; \n  uncount(count) |&gt; \n  pull(n_syllables) |&gt; \n  fitdistrplus::fitdist(distr = 'pois', method = 'mle')\n\nFitting of the distribution ' pois ' by maximum likelihood \nParameters:\n       estimate Std. Error\nlambda 2.782715 0.02128182\n\n\n\nЕсть и другие методы оценки параметров.\nМетод максимального правдоподобия может быть чувствителен к размеру выборки.\n\n\n\n\n\n\n\nОтфильтруйте из данных с количеством слогов в андийских языках багвалинский и, используя метод максимального правдоподобия, оцените для них параметры модели Пуассона.\n\n\n\n\n\n\n\n\n\nВ работе (Coretta 2016) собраны данные длительности исландских гласных. Отфильтруйте данные, оставив односложные слова (переменная syllables) после придыхательного (переменная aspiration), произнесенные носителем tt01 (переменная speaker) и постройте следующий график, моделируя длительность гласных (переменная vowel.dur) нормальным и логнормальным распределением. Как вам кажется, какое распределение лучше подходит к данным? Докажите ваше утверждение, сравнив значения правдоподобия.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Метод максимального правдоподобия</span>"
    ]
  },
  {
    "objectID": "02-likelihood.html#логорифм-функции-правдоподобия",
    "href": "02-likelihood.html#логорифм-функции-правдоподобия",
    "title": "2  Метод максимального правдоподобия",
    "section": "2.5 Логорифм функции правдоподобия",
    "text": "2.5 Логорифм функции правдоподобия\nТак как в большинстве случаев нужно найти лишь максимум функции правдоподобия, а не саму функцию \\(\\ell(x|\\theta)\\), то для облегчения подсчетов используют логорифмическую функцию правдоподобия \\(\\ln\\ell(x|\\theta)\\): в результате, вместо произведения появляется сумма1:\n\\[\\text{argmax}_\\theta \\prod \\ell(\\theta|x) = \\text{argmax}_\\theta \\sum \\ln\\ell(\\theta|x) \\]\nВо всех предыдущих примерах мы смотрели на 1–3 примера данных, давайте попробуем использовать функцию правдоподобия для большего набора данных.\n\n\n\n\n\n\nПредставим, что мы проводим некоторый эксперимент, и у некоторых участников все получается с первой попытки, а некоторым нужна еще одна попытка или даже две. Дополните код функциями правдоподобия и логорифмической функцией правдоподобия, чтобы получился график ниже.\n\n\n\n\nset.seed(42)\nv &lt;- sample(0:2, 10, replace = TRUE)\n\nsapply(seq(0.01, 0.99, 0.01), function(p){\n  ...\n}) -&gt;\n  likelihood\n\nsapply(seq(0.01, 0.99, 0.01), function(p){\n  ...\n}) -&gt;\n  loglikelihood\n\ntibble(p = seq(0.01, 0.99, 0.01),\n       loglikelihood,\n       likelihood) |&gt; \n  pivot_longer(names_to = \"type\", values_to = \"value\", loglikelihood:likelihood) |&gt; \n  ggplot(aes(p, value))+\n  geom_line()+\n  geom_vline(xintercept = 0.33, linetype = 2)+\n  facet_wrap(~type, scales = \"free_y\", nrow = 2)+\n  scale_x_continuous(breaks = c(0:5*0.25, 0.33))",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Метод максимального правдоподобия</span>"
    ]
  },
  {
    "objectID": "02-likelihood.html#послесловие",
    "href": "02-likelihood.html#послесловие",
    "title": "2  Метод максимального правдоподобия",
    "section": "2.6 Послесловие",
    "text": "2.6 Послесловие\n\nНе стоит путать метод максимального правдоподобия (MLE) c градиентным спуском\nМетод максимального правдоподобия — не единственный способ оценить параметр, смотрите, например, Maximum Spacing Estimation (Cheng and Amin 1983; Ranneby 1984; Anatolyev and Kosenok 2005), см. msedist() из пакета fitdistrplus.\n\n\n\n\n\nAnatolyev, Stanislav, and Grigory Kosenok. 2005. “An Alternative to Maximum Likelihood Based on Spacings.” Econometric Theory 21 (2): 472–76.\n\n\nCheng, R. C. H., and N. A. K. Amin. 1983. “Estimating Parameters in Continuous Univariate Distributions with a Shifted Origin.” Journal of the Royal Statistical Society: Series B (Methodological) 45 (3): 394–403.\n\n\nCoretta, Stefano. 2016. “Vowel Duration and Aspiration Effects in Icelandic.” University of York.\n\n\nHillenbrand, James, Laura A Getty, Michael J Clark, and Kimberlee Wheeler. 1995. “Acoustic Characteristics of American English Vowels.” The Journal of the Acoustical Society of America 97 (5): 3099–3111.\n\n\nRanneby, Bo. 1984. “The Maximum Spacing Method. An Estimation Method Related to the Maximum Likelihood Method.” Scandinavian Journal of Statistics, 93–112.\n\n\nRosenbach, Anette. 2003. “Aspects of Iconicity and Economy in the Choice Between the s-Genitive and the of-Genitive in English.” In Determinants of Grammatical Variation in English, edited by Günter Rohdenburg and Britta Mondorf. Berlin, New York: Mouton de Gruyter.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Метод максимального правдоподобия</span>"
    ]
  },
  {
    "objectID": "02-likelihood.html#footnotes",
    "href": "02-likelihood.html#footnotes",
    "title": "2  Метод максимального правдоподобия",
    "section": "",
    "text": "Это просто свойство логарифмов: log(5*5) = log(5)+log(5)↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Метод максимального правдоподобия</span>"
    ]
  },
  {
    "objectID": "03-mixture-models.html#cмеси-распределений",
    "href": "03-mixture-models.html#cмеси-распределений",
    "title": "3  Модели смеси распределений",
    "section": "3.1 Cмеси распределений",
    "text": "3.1 Cмеси распределений\nНе все переменные выглядят так же красиво, как распределения из учебников статистики. Для примера возьмем датасет, который содержит спамерские и обычные смс-сообщения, выложенный UCI Machine Learning на kaggle. Посчитаем количество символов в сообщениях:\n\nspam_sms &lt;- read_csv(\"https://raw.githubusercontent.com/agricolamz/2024_HSE_b_da4l/main/data/spam_sms.csv\")\n\nglimpse(spam_sms)\n\nRows: 5,572\nColumns: 2\n$ type    &lt;chr&gt; \"ham\", \"ham\", \"spam\", \"ham\", \"ham\", \"spam\", \"ham\", \"ham\", \"spa…\n$ message &lt;chr&gt; \"Go until jurong point, crazy.. Available only in bugis n grea…\n\nspam_sms %&gt;% \n  mutate(n_char = nchar(message)) -&gt;\n  spam_sms\n  \nglimpse(spam_sms)\n\nRows: 5,572\nColumns: 3\n$ type    &lt;chr&gt; \"ham\", \"ham\", \"spam\", \"ham\", \"ham\", \"spam\", \"ham\", \"ham\", \"spa…\n$ message &lt;chr&gt; \"Go until jurong point, crazy.. Available only in bugis n grea…\n$ n_char  &lt;int&gt; 111, 29, 155, 49, 61, 147, 77, 160, 157, 154, 109, 136, 155, 1…\n\nspam_sms %&gt;% \n  ggplot(aes(n_char))+\n  geom_histogram(fill = \"gray90\")+\n  labs(caption = \"данные из kaggle.com/uciml/sms-spam-collection-dataset\",\n       x = \"количество символов\",\n       y = \"значение функции плотности\")\n\n\n\n\n\n\n\n\nМы видим два явных горба и, как можно догадаться, это связано с тем, что спамерские сообщения в среднем длиннее и сосредоточены вокруг ограничения смс в 160 символов:\n\nspam_sms %&gt;% \n  ggplot(aes(n_char))+\n  geom_histogram(fill = \"gray70\", aes(y = after_stat(density)))+\n  geom_density(aes(fill = type), alpha = 0.3)+\n  labs(caption = \"данные из kaggle.com/uciml/sms-spam-collection-dataset\",\n       x = \"количество символов\",\n       y = \"значение функции плотности\")+\n  geom_vline(xintercept = 160, linetype = 2, size = 0.3)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Модели смеси распределений</span>"
    ]
  },
  {
    "objectID": "03-mixture-models.html#модели-смеси-распределений",
    "href": "03-mixture-models.html#модели-смеси-распределений",
    "title": "3  Модели смеси распределений",
    "section": "3.2 Модели смеси распределений",
    "text": "3.2 Модели смеси распределений\nТакого рода данные можно описать при помощи модели смеси разных распределений. Мы сейчас опишем нормальными распределениями и будем использовать пакет mixtools (для смесей нормальных распределений лучше использовать пакет mclust), но, ясно, что семейство распределений можно было бы подобрать и получше.\n\nlibrary(mixtools)\n\nset.seed(42)\nspam_length_est &lt;- normalmixEM(spam_sms$n_char)\n\nnumber of iterations= 73 \n\nsummary(spam_length_est)\n\nsummary of normalmixEM object:\n          comp 1     comp 2\nlambda  0.439334   0.560666\nmu     37.858905 114.070490\nsigma  13.398985  60.921536\nloglik at estimate:  -29421.36 \n\n\nКласс, получаемый в результате работы функции normalmixEM() имеет встроеный график:\n\nplot(spam_length_est, density = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nОднако, если хочется больше контроля над получаемым разультатом, я бы предложил использовать ggplot():\n\nnew_dnorm &lt;- function(x, mu, sigma, lambda){\n  dnorm(x, mu, sigma)*lambda\n}\n\nspam_sms %&gt;% \n  ggplot(aes(n_char))+\n  geom_histogram(aes(y = after_stat(density)), fill = \"gray90\")+\n  stat_function(fun = new_dnorm, \n                args = c(mu = spam_length_est$mu[1],\n                                          sigma = spam_length_est$sigma[1],\n                                          lambda = spam_length_est$lambda[1]),\n                color = \"#F8766D\")+\n  stat_function(fun = new_dnorm, \n                args = c(mu = spam_length_est$mu[2],\n                                          sigma = spam_length_est$sigma[2],\n                                          lambda = spam_length_est$lambda[2]),\n                color = \"#00BFC4\")+\n  labs(caption = \"данные из kaggle.com/uciml/sms-spam-collection-dataset\",\n       x = \"количество символов\",\n       y = \"значение функции плотности\")+\n  geom_vline(xintercept = 160, linetype = 2, size = 0.3)\n\n\n\n\n\n\n\n\nТаким образом мы получили классификатор\n\nfirst &lt;- new_dnorm(seq(1, 750, by = 1), \n                   mu = spam_length_est$mu[1],\n                   sigma = spam_length_est$sigma[1],\n                   lambda = spam_length_est$lambda[1])\nsecond &lt;- new_dnorm(seq(1, 750, by = 1), \n                    mu = spam_length_est$mu[2],\n                    sigma = spam_length_est$sigma[2],\n                    lambda = spam_length_est$lambda[2])\nwhich(first &gt; second)\n\n [1]  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30\n[26] 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55\n[51] 56 57 58 59 60 61 62\n\n\nЕсли в смс-сообщении больше 62 символов, то согласно нашей модели, вероятнее всего это спам.\n\nspam_sms %&gt;% \n  mutate(model_predict = ifelse(n_char &gt; 63, \"predicted_spam\", \"predicted_ham\")) %&gt;% \n  count(model_predict, type) %&gt;% \n  pivot_wider(names_from = type, values_from = n)\n\n# A tibble: 2 × 3\n  model_predict    ham  spam\n  &lt;chr&gt;          &lt;int&gt; &lt;int&gt;\n1 predicted_ham   2834    25\n2 predicted_spam  1991   722\n\n\nРезультат не идеальный, но лучше чем помечать как спам каждое 13 сообщение \\(747/(4825+747)\\).\n\n\n\n\n\n\nВ работе (Coretta 2016) собраны данные длительности исландских гласных. Отфильтруйте данные, оставив наблюдения гласного [a] (переменная vowel), произнесенные носителем tt01 (переменная speaker) и постройте следующие графики, моделируя длительность гласного (переменная vowel.dur) смесью трех нормальных распределений. Как вам кажется, насколько хорошо модель смеси справилась с заданием?\n\n\n\n\n\nnumber of iterations= 114",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Модели смеси распределений</span>"
    ]
  },
  {
    "objectID": "03-mixture-models.html#несколько-замечаний",
    "href": "03-mixture-models.html#несколько-замечаний",
    "title": "3  Модели смеси распределений",
    "section": "3.3 Несколько замечаний",
    "text": "3.3 Несколько замечаний\n\nВ наших примерах нам была доступна информация о классах (spam/ham, coronal/labial/velar), однако модель смесей распределений как раз имеет смысл применять, когда такой информации нет.\nВ смеси распределений может быть любое количество распределений.\nМодели смеси распределений не ограничены только нормальным распределением, алгоритм можно использовать и для других распределений.\nЧаще всего в моделях смеси распределений используются распределения одного семейства, однако можно себе представить и комбинации посложнее.\nМодели смеси распределений (mixture models) не стоит путать со смешанными моделями (mixed effects models).\n\n\n\n\n\nCoretta, Stefano. 2016. “Vowel Duration and Aspiration Effects in Icelandic.” University of York.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Модели смеси распределений</span>"
    ]
  },
  {
    "objectID": "04-intro-to-Bayes.html#нотация",
    "href": "04-intro-to-Bayes.html#нотация",
    "title": "4  Байесовский статистический вывод",
    "section": "4.1 Нотация",
    "text": "4.1 Нотация\nВ байесовском подходе статистический вывод описывается формулой Байеса\n\\[P(θ|Data) = \\frac{P(Data|θ)\\times P(θ)}{P(Data)}\\]\n\n\\(P(θ|Data)\\) — апостериорная вероятность (posterior)\n\\(P(Data|θ)\\) — функция правдоподобия (likelihood)\n\\(P(θ)\\) — априорная вероятность (prior)\n\\(P(Data)\\) — нормализующий делитель\n\nВ литературе можно еще встретить такую запись:\n\\[P(θ|Data) \\propto P(Data|θ)\\times P(θ)\\]\nНа прошлых занятиях мы говорили, что функция правдоподобия не обязана интегрироваться до 1, тогда почему, назвав часть формулы Байеса \\(P(Data|θ)\\) функцией правдоподобия, мы оставляем нотацию, будто это функция вероятностей? Потому что это условная вероятность, она не обязана интегрироваться до 1.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Байесовский статистический вывод</span>"
    ]
  },
  {
    "objectID": "04-intro-to-Bayes.html#категориальный-пример",
    "href": "04-intro-to-Bayes.html#категориальный-пример",
    "title": "4  Байесовский статистический вывод",
    "section": "4.2 Категориальный пример",
    "text": "4.2 Категориальный пример\nДля примера я взял датасет, который содержит спамерские и обычные смс-сообщения, выложенный UCI Machine Learning на kaggle и при помощи пакета udpipe токенизировал и определил часть речи:\n\nsms_pos &lt;- read_csv(\"https://raw.githubusercontent.com/agricolamz/2024_HSE_b_da4l/main/data/spam_sms_pos.csv\")\nglimpse(sms_pos)\n\nRows: 34\nColumns: 3\n$ type &lt;chr&gt; \"ham\", \"ham\", \"ham\", \"ham\", \"ham\", \"ham\", \"ham\", \"ham\", \"ham\", \"h…\n$ upos &lt;chr&gt; \"ADJ\", \"ADP\", \"ADV\", \"AUX\", \"CCONJ\", \"DET\", \"INTJ\", \"NOUN\", \"NUM\"…\n$ n    &lt;dbl&gt; 4329, 5004, 5832, 5707, 1607, 3493, 1676, 12842, 1293, 2424, 1144…\n\nsms_pos |&gt; \n  group_by(type) |&gt; \n  mutate(ratio = n/sum(n),\n         upos = fct_reorder(upos, n, mean, .desc = TRUE)) |&gt;\n  ggplot(aes(type, ratio))+\n  geom_col()+\n  geom_label(aes(label = round(ratio, 3)), position = position_stack(vjust = 0.5))+\n  facet_wrap(~upos, scales = \"free_y\")\n\n\n\n\n\n\n\n\nДавайте полученные доли считать нашей моделью: сумма всех чисел внутри каждого типа (ham/spam) дает в сумме 1. Мы получили новое сообщение:\n\nCall FREEPHONE 0800 542 0825 now!\n\nМодель udpipe разобрала его следующим образом:\n\nVERB NUM NUM NUM NUM ADV PUNCT\n\nПонятно, что это – спам, но мы попытаемся применить байесовский статистический вывод, чтобы определить тип сообщения. Предположим, что машина считает обе гипотезы равновероятными, т. е. ее априорное распределение гипотез равно 0.5 каждая. На минуту представим, что машина анализирует текст пословно. Первое слово типа VERB. Функции правдоподобия равны 0.135 и 0.096 для сообщений типа ham и spam соответственно. Применим байесовский апдейт:\n\ntibble(model = c(\"ham\", \"spam\"),\n       prior = 0.5,\n       likelihood = c(0.135, 0.096),\n       product = prior*likelihood,\n       posterior = product/sum(product))\n\n# A tibble: 2 × 5\n  model prior likelihood product posterior\n  &lt;chr&gt; &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1 ham     0.5      0.135  0.0675     0.584\n2 spam    0.5      0.096  0.048      0.416\n\n\nВот мы и сделали байесовский апдейт. Теперь апостериорное распределение, которое мы получили на предыдущем шаге, мы можем использовать в новом апдейте. Следующее слово в сообщении типа NUM.\n\ntibble(model = c(\"ham\", \"spam\"),\n       prior_2 = c(0.584, 0.416),\n       likelihood_2 = c(0.016, 0.117),\n       product_2 = prior_2*likelihood_2,\n       posterior_2 = product_2/sum(product_2))\n\n# A tibble: 2 × 5\n  model prior_2 likelihood_2 product_2 posterior_2\n  &lt;chr&gt;   &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 ham     0.584        0.016   0.00934       0.161\n2 spam    0.416        0.117   0.0487        0.839\n\n\nУже на второй итерации, наша модель почти уверена, что это сообщение spam. На третьей итерации уверенность только растет:\n\ntibble(model = c(\"ham\", \"spam\"),\n       prior_3 = c(0.161, 0.839),\n       likelihood_3 = c(0.016, 0.117),\n       product_3 = prior_3*likelihood_3,\n       posterior_3 = product_3/sum(product_3))\n\n# A tibble: 2 × 5\n  model prior_3 likelihood_3 product_3 posterior_3\n  &lt;chr&gt;   &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 ham     0.161        0.016   0.00258      0.0256\n2 spam    0.839        0.117   0.0982       0.974 \n\n\n\n\n\n\n\n\nНа основе первых трех слов посчитайте посчитайте вероятность гипотезы, что перед нами спамерское сообщение, если предположить, что каждое пятое сообщение – спам. Ответ округлите до трех знаков после запятой.\n\n\n\n\n\n\n\n\n\n\n\n\n\nИз формулы Байеса следует, что не обязательно каждый раз делить на нормализующий делитель, это можно сделать единожды.\n\ntibble(model = c(\"ham\", \"spam\"),\n       prior = 0.5,\n       likelihood = c(0.135, 0.096),\n       likelihood_2 = c(0.016, 0.117),\n       product = prior*likelihood*likelihood_2*likelihood_2,\n       posterior = product/sum(product))\n\n# A tibble: 2 × 6\n  model prior likelihood likelihood_2   product posterior\n  &lt;chr&gt; &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 ham     0.5      0.135        0.016 0.0000173    0.0256\n2 spam    0.5      0.096        0.117 0.000657     0.974 \n\n\nИз приведенных рассуждений также следует, что все равно в каком порядке мы производим байесовский апдейт: мы могли сначала умножить на значение правдоподобия для категории NUM и лишь в конце на значение правдоподобия VERB.\nТакже стоит отметить, что если данных много, то через какое-то время становится все равно, какое у нас было априорное распределение. Даже в нашем примере, в котором мы проанализировали первые три слова сообщения, модель, прогнозирующая, что сообщение спамерское, выиграет, даже если, согласно априорному распределению, спамерским является каждое 20 сообщение:\n\ntibble(model = c(\"ham\", \"spam\"),\n       prior = c(0.95, 0.05),\n       likelihood = c(0.135, 0.096),\n       likelihood_2 = c(0.016, 0.117),\n       product = prior*likelihood*likelihood_2*likelihood_2,\n       posterior = product/sum(product))\n\n# A tibble: 2 × 6\n  model prior likelihood likelihood_2   product posterior\n  &lt;chr&gt; &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 ham    0.95      0.135        0.016 0.0000328     0.333\n2 spam   0.05      0.096        0.117 0.0000657     0.667\n\n\nСамым главным отличием байесовского статистического вывода от фриквентистского, является то, что мы в результате получаем вероятность каждой из моделей. Это очень значительно отличается от фриквентистской практики нулевых гипотез и p-value, в соответствии с которыми мы можем лишь отвергнуть или не отвергнуть нулевую гипотезу.\n\n\n\n\n\n\nВашего друга похитили а на почту отправили датасет, в котором записаны данные о погоде из пяти городов. Ваш телефон зазвонил, и друг сказал, что не знает куда его похитили, но за окном легкий дождь (Rain). А в какой-то из следующих дней — сильный дождь (Rain, Thunderstorm). Исходя из явно неверного предположения, что погодные условия каждый день не зависят друг от друга, сделайте байесовский апдейт и предположите, в какой город вероятнее всего похитили друга.\n\n\n\n\n\n\n\nAuckland\nBeijing\nChicago\nMumbai\nSan Diego\n\n\n\n\n\n\n\n\n\n\n\n\n\nУкажите получившуюся вероятность. Выполняя задание, округлите все вероятности и значения правдоподобия до 3 знаков после запятой.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Байесовский статистический вывод</span>"
    ]
  },
  {
    "objectID": "04-intro-to-Bayes.html#разница-между-фриквентиским-и-байесовским-подходами",
    "href": "04-intro-to-Bayes.html#разница-между-фриквентиским-и-байесовским-подходами",
    "title": "4  Байесовский статистический вывод",
    "section": "4.3 Разница между фриквентиским и байесовским подходами",
    "text": "4.3 Разница между фриквентиским и байесовским подходами\n\nКартинка из одной из моих любимых книг по статистике (Efron and Hastie 2016: 34).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Байесовский статистический вывод</span>"
    ]
  },
  {
    "objectID": "04-intro-to-Bayes.html#биномиальные-данные",
    "href": "04-intro-to-Bayes.html#биномиальные-данные",
    "title": "4  Байесовский статистический вывод",
    "section": "4.4 Биномиальные данные",
    "text": "4.4 Биномиальные данные\nБиномиальные данные возникают, когда нас интересует доля успехов в какой-то серии эксперементов Бернулли.\n\n4.4.1 Биномиальное распределение\nБиномиальное распределение — распределение количества успехов эксперементов Бернулли из n попыток с вероятностью успеха p.\n\\[P(k | n, p) = \\frac{n!}{k!(n-k)!} \\times p^k \\times (1-p)^{n-k} =  {n \\choose k} \\times p^k \\times (1-p)^{n-k}\\] \\[ 0 \\leq p \\leq 1; n, k &gt; 0\\]\n\ntibble(x = 0:50,\n           density = dbinom(x = x, size = 50, prob = 0.16)) |&gt; \n  ggplot(aes(x, density))+\n  geom_point()+\n  geom_line()+\n  labs(title = \"Биномиальное распределение p = 0.16, n = 50\")\n\n\n\n\n\n\n\n\n\n\n4.4.2 Бета распределение\n\\[P(x; α, β) = \\frac{x^{α-1}\\times (1-x)^{β-1}}{B(α, β)}; 0 \\leq x \\leq 1; α, β &gt; 0\\]\nБета функция:\n\\[Β(α, β) = \\frac{Γ(α)\\times Γ(β)}{Γ(α+β)} = \\frac{(α-1)!(β-1)!}{(α+β-1)!} \\]\n\ntibble(x = seq(0, 1, length.out = 100),\n           density = dbeta(x = x, shape1 = 8, shape2 = 42)) |&gt; \n  ggplot(aes(x, density))+\n  geom_point()+\n  geom_line()+\n  labs(title = \"Бета распределение α = 8, β = 42\")\n\n\n\n\n\n\n\n\nМожно поиграть с разными параметрами:\n\nshiny::runGitHub(\"agricolamz/beta_distribution_shiny\") \n\n\\[\\mu = \\frac{\\alpha}{\\alpha+\\beta}\\]\n\\[\\sigma^2 = \\frac{\\alpha\\times\\beta}{(\\alpha+\\beta)^2\\times(\\alpha+\\beta+1)}\\]\n\n\n4.4.3 Байесовский апдейт биномиальных данных\n\\[Beta_{post}(\\alpha_{post}, \\beta_{post}) = Beta(\\alpha_{prior}+\\alpha_{data}, \\beta_{prior}+\\beta_{data}),\\] где \\(Beta\\) — это бета распределение\n\nshiny::runGitHub(\"agricolamz/bayes_for_binomial_app\") \n\n\n\n\n\n\n\nНемного упрощая данные из статьи (Rosenbach 2003: 394), можно сказать что носители британского английского предпочитают s-генитив (90%) of-генитиву (10%). Проведите байесовский апдейт, если Вы наблюдаете в интервью британского актера из 120 контекстов 92 s-генитивов. Априорное распределение берите соразмерное данным. Ответ округлите до трёх или менее знаков после запятой.\n\n\n\n\n\n\n\n\n\nПараметр альфа:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nПараметр бета:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.4.4 Байесовский апдейт биномиальных данных: несколько моделей\n\ntibble(x = rep(seq(0, 1, length.out = 100), 6),\n           density = c(dbeta(unique(x), shape1 = 8, shape2 = 42),\n                       dbeta(unique(x), shape1 = 16, shape2 = 34),\n                       dbeta(unique(x), shape1 = 24, shape2 = 26),\n                       dbeta(unique(x), shape1 = 8+4, shape2 = 42+16),\n                       dbeta(unique(x), shape1 = 16+4, shape2 = 34+16),\n                       dbeta(unique(x), shape1 = 24+4, shape2 = 26+16)),\n           type = rep(c(\"prior\", \"prior\", \"prior\", \"posterior\", \"posterior\", \"posterior\"), each = 100),\n           dataset = rep(c(\"prior: 8, 42\", \"prior: 16, 34\", \"prior: 24, 26\",\n                           \"prior: 8, 42\", \"prior: 16, 34\", \"prior: 24, 26\"), each = 100)) |&gt; \n  ggplot(aes(x, density, color = type))+\n  geom_line()+\n  facet_wrap(~dataset)+\n  labs(title = \"data = 4, 16\")\n\n\n\n\n\n\n\n\n\n\n4.4.5 Что почитать?\nЕсли остались неясности, то можно посмотреть 2-ую главу (Robinson 2017).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Байесовский статистический вывод</span>"
    ]
  },
  {
    "objectID": "04-intro-to-Bayes.html#байесовский-апдейт-нормального-распределения",
    "href": "04-intro-to-Bayes.html#байесовский-апдейт-нормального-распределения",
    "title": "4  Байесовский статистический вывод",
    "section": "4.5 Байесовский апдейт нормального распределения",
    "text": "4.5 Байесовский апдейт нормального распределения\nВстроенный датасет ChickWeight содержит вес цыплят (weight) в зависимости от типа диеты (Diet). Мы будем анализировать 20-дневных птенцов.\n\nChickWeight |&gt; \n  filter(Time == 20) -&gt;\n  chicks\n\nchicks |&gt; \n  ggplot(aes(weight))+\n  geom_density()\n\n\n\n\n\n\n\n\nНачнем с апостериорных параметров для наблюдений \\(x_1, ... x_n\\) со средним \\(\\mu_{data}\\) известной дисперсией \\(\\sigma_{known}^2\\)\n\n4.5.1 Байесовский апдейт нормального распределения: выбор из нескольких моделей\nМы можем рассматривать эту задачу как выбор между несколькими моделями с разными средними:\n\ntibble(x = rep(1:400, 6),\n           density = c(dnorm(unique(x), mean = 125, sd = 70),\n                       dnorm(unique(x), mean = 150, sd = 70),\n                       dnorm(unique(x), mean = 175, sd = 70),\n                       dnorm(unique(x), mean = 200, sd = 70),\n                       dnorm(unique(x), mean = 225, sd = 70),\n                       dnorm(unique(x), mean = 250, sd = 70)),\n           dataset = rep(1:6, each = 400)) |&gt; \n  ggplot(aes(x, density, color = factor(dataset)))+\n  geom_line()\n\n\n\n\n\n\n\n\nДальше мы можем точно так же апдейтить, как мы делали раньше:\n\ntibble(mu = seq(125, 250, by = 25),\n           prior = 1/6,\n           likelihood = c(prod(dnorm(chicks$weight, mean = 125, sd = 70)),\n                          prod(dnorm(chicks$weight, mean = 150, sd = 70)),\n                          prod(dnorm(chicks$weight, mean = 175, sd = 70)),\n                          prod(dnorm(chicks$weight, mean = 200, sd = 70)),\n                          prod(dnorm(chicks$weight, mean = 225, sd = 70)),\n                          prod(dnorm(chicks$weight, mean = 250, sd = 70))),\n           product = prior*likelihood,\n           posterior = product/sum(product)) -&gt;\n  results\nresults\n\n# A tibble: 6 × 5\n     mu prior likelihood   product posterior\n  &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1   125 0.167  2.06e-127 3.44e-128  2.39e-15\n2   150 0.167  4.74e-120 7.90e-121  5.48e- 8\n3   175 0.167  3.08e-115 5.13e-116  3.56e- 3\n4   200 0.167  5.66e-113 9.43e-114  6.55e- 1\n5   225 0.167  2.95e-113 4.91e-114  3.41e- 1\n6   250 0.167  4.34e-116 7.23e-117  5.02e- 4\n\nresults |&gt; \n  select(mu, prior, posterior) |&gt; \n  pivot_longer(names_to = \"type\", values_to = \"probability\", prior:posterior) |&gt; \n  ggplot(aes(mu, probability, color = type))+\n  geom_point()+\n  labs(title = \"изменение вероятностей для каждой из моделей\",\n       x = \"μ\")\n\n\n\n\n\n\n\n\n\n\n4.5.2 Байесовский апдейт нормального распределения: непрерывный вариант\nВо первых, нам понадобится некоторая мера, которая называется точность (precision):\n\\[\\tau = \\frac{1}{\\sigma^2}\\]\n\\[\\tau_{post} = \\tau_{prior} + \\tau_{data} \\Rightarrow \\sigma^2_{post} = \\frac{1}{\\tau_{post}}\\]\n\\[\\mu_{post} = \\frac{\\mu_{prior} \\times \\tau_{prior} + \\mu_{data} \\times \\tau_{data}}{\\tau_{post}}\\]\nТак что если нашим априорным распределением мы назовем нормальное распределение со средним около 180 и стандартным отклонением 90, то процесс байесовского апдейта будет выглядеть вот так:\n\nsd_prior &lt;- 90 \nsd_data &lt;- sd(chicks$weight)\nsd_post &lt;- 1/sqrt(1/sd_prior^2 + 1/sd_data^2)\nmean_prior &lt;- 180\nmean_data &lt;- mean(chicks$weight)\nmean_post &lt;- weighted.mean(c(mean_prior, mean_data), c(1/sd_prior^2, 1/sd_data^2))\n\nchicks |&gt; \n  ggplot(aes(weight)) +\n  geom_histogram(aes(y = after_stat(density)))+\n  stat_function(fun = dnorm, args = list(mean_prior,  sd_prior), color = \"lightblue\")+\n  stat_function(fun = dnorm, args = list(mean_post,  sd_post), color = \"red\")\n\n\n\n\n\n\n\n\n\nshiny::runGitHub(\"agricolamz/bayes_for_normal_app\") \n\n\n\n\n\n\n\nВ работе (Coretta 2016) собраны данные длительности исландских гласных. Отфильтруйте данные, произнесенные носителем tt01 (переменная speaker), произведите байесовский апдейт данных, моделируя длительность гласных (переменная vowel.dur) нормальным распределением и постройте график. В качестве априорного распределения используйте нормальное распределение со средним 87 и стандартным отклонением 25.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.5.3 Что почитать?\n\nMurphy K. P. (2007) Conjugate Bayesian analysis of the Gaussian distribution\nJordan M. I. (2010) The Conjugate Prior for the Normal Distribution\nраздел 2.5 в Gelman A. et. al (2014) Bayesian Data Analysis",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Байесовский статистический вывод</span>"
    ]
  },
  {
    "objectID": "04-intro-to-Bayes.html#другие-распределения",
    "href": "04-intro-to-Bayes.html#другие-распределения",
    "title": "4  Байесовский статистический вывод",
    "section": "4.6 Другие распределения",
    "text": "4.6 Другие распределения\nМы обсудили биномиальные и нормальнораспределенные данные. Так случилось, что для них есть короткий путь сделать байесовский апдейт, не применяя формулы байеса. И нам так повезло, что связки априорного/апосториорного распределений и функции правдоподобия такие простые:\n\nаприорного/апосториорного распределены как бета распределение, значит функция правдоподобия – биномиальное распределение\nесли мы моделируем данные при помощи нормального распределения, то все три распределения (априорное, функция правдопдобия и апосториорное) – нормальные.\n\nТакие отношения между распределениями называют сопряженными (conjugate). В результате для разных семейств функции правдоподобия существует список соответствующих сопряженных априорных распределений (conjugate prior), который можно найти, например, здесь.\nВ более случаях используется (а на самом деле почти всегда) Марковские цепи Монте-Карло (MCMC).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Байесовский статистический вывод</span>"
    ]
  },
  {
    "objectID": "04-intro-to-Bayes.html#вопросы-к-апостериорному-распределению",
    "href": "04-intro-to-Bayes.html#вопросы-к-апостериорному-распределению",
    "title": "4  Байесовский статистический вывод",
    "section": "4.7 Вопросы к апостериорному распределению",
    "text": "4.7 Вопросы к апостериорному распределению\n\nA frequentist uses impeccable logic to answer the wrong question, while a Bayesian answers the right question by making assumptions that nobody can fully believe in. (P. G. Hammer)\n\n\nпопытка оценить параметр θ и/или какой-нибудь интервал, в котором он лежит.\n\nсреднее апостериорного распределения (mean of the posterior estimation, MAP)\nмаксимум апостериорного распределения (maximum a posteriori estimation, MAP)\nбайесовский доверительный интервал\n\nответить на вопросы вроде\n\nкакова вероятность, что значение θ больше некоторого значения \\(x\\)?\nкакова вероятность, что значение θ лежит в интервале \\([x; y]\\)?\nи т. п.\n\nВыборки из апостериорного распределения (Posterior simulation):\n\nсимулируйте большую выборку из апостериорного распределения;\nиспользуйте полученную выборку для статистического вывода.\n\n\nДопустим, мы получили апостериорное бета распределение с параметрами 20 и 70. Какова вероятность наблюдать значения больше 0.3?\n\nposterior_simulation &lt;- rbeta(n = 10000, shape1 = 20, shape2 = 70)\nsum(posterior_simulation &gt; 0.3)/10000\n\n[1] 0.04\n\n\nИ это не p-value! Это настоящие вероятности!\n\n\n\n\nCoretta, Stefano. 2016. “Vowel Duration and Aspiration Effects in Icelandic.” University of York.\n\n\nEfron, Bradley, and Trevor Hastie. 2016. Computer Age Statistical Inference. Vol. 5. Cambridge University Press.\n\n\nRobinson, D. 2017. Introduction to Empirical Bayes: Examples from Baseball Statistics. ASIN: B06WP26J8Q.\n\n\nRosenbach, Anette. 2003. “Aspects of Iconicity and Economy in the Choice Between the s-Genitive and the of-Genitive in English.” In Determinants of Grammatical Variation in English, edited by Günter Rohdenburg and Britta Mondorf. Berlin, New York: Mouton de Gruyter.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Байесовский статистический вывод</span>"
    ]
  }
]